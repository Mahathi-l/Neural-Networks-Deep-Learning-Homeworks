{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hw06.ipynb","provenance":[{"file_id":"1HAsn_X1X8MEiUBne_OuHDPHKUmaIZ9gr","timestamp":1634026313867},{"file_id":"1DDKNNUBHxVWifAekYT2onwSCAXAwmbjI","timestamp":1633979537915}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNaxCFz5AzCZTyKIBfq/MrJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"U9wxy2h3pRpT"},"source":["# Exploring RNNs\n","\n","In this assignment, you will be asked to modify the [notebook](https://colab.research.google.com/drive/1Ge7HNinj0riX56ayukvbVpKpg-BFgni0?usp=sharing) which we went over in class exploring the use of RNNs.\n","\n","We begin by donwloading and unziping the dataset."]},{"cell_type":"code","metadata":{"id":"4MeLUtffcOIS"},"source":["!wget https://download.pytorch.org/tutorial/data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsQZlBLDcOvS"},"source":["!unzip data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lvAM1atachCu"},"source":["## Loading and Formatting the Data\n","\n","We provide helper functions for loading the data, and store it as a dictionary with entries for each nationality. Data is split into training and test as well."]},{"cell_type":"code","metadata":{"id":"7u4FEq3Ech2t"},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import glob\n","import os\n","import unicodedata\n","import string\n","import torch\n","\n","# This function returns the path of the files in the dataset\n","def findFiles(path): return glob.glob(path)\n","\n","# Specifying list of characters\n","all_letters = string.ascii_letters + \" .,;'\"\n","n_letters = len(all_letters)\n","\n","# This function converts unicode to ascii\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","        and c in all_letters\n","    )\n","\n","# Function for reading afile and splitting into lines\n","def readLines(filename):\n","    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n","    return [unicodeToAscii(line) for line in lines]\n","\n","# Specifying percentage of data used for training\n","perTrain = 0.9\n","\n","# Build the category_lines dictionary, a list of names per language\n","category_lines_train = {}\n","category_lines_test = {}\n","all_categories = []\n","\n","# Loading all the data\n","for filename in findFiles('data/names/*.txt'):\n","    category = os.path.splitext(os.path.basename(filename))[0]\n","    all_categories.append(category)\n","    lines = readLines(filename)\n","    category_lines_train[category] = lines[0:int(perTrain*len(lines))]\n","    category_lines_test[category] = lines[int(perTrain*len(lines)):]\n","\n","# Specifying the number of categories\n","n_categories = len(all_categories)\n","\n","# Find letter index from all_letters, e.g. \"a\" = 0\n","def letterToIndex(letter):\n","    return all_letters.find(letter)\n","\n","# Turn a line into a <line_length x 1 x n_letters>,\n","# or an array of one-hot letter vectors\n","def lineToTensor(line):\n","    tensor = torch.zeros(len(line), 1, n_letters)\n","    for li, letter in enumerate(line):\n","        tensor[li][0][letterToIndex(letter)] = 1\n","    return tensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gG5TaXZrlq7f"},"source":["## [Task 1] Using Adam Optimizer [30 pts]\n","\n","We will be replacing the implementation of standard gradient descent in the baseline model by a call of the Adam optimizer. Do the following:\n","\n","1. [12 pts] Train the baseline model with the standard gradient descent and a version using Adam optimizer. Plot the learning curves for both approaches. Train both models for only 50,000 iterations.\n","2. [12 pts] Evaluate the performance of both models on the test set.\n","3. [6 pts] Comment on the performance of both methods on the test set, and the shape of their learning curves.\n"]},{"cell_type":"markdown","metadata":{"id":"2n2n08UbgrYO"},"source":["### Task 1.1"]},{"cell_type":"code","metadata":{"id":"UgFFTkqod2Lt"},"source":["### TO DO - Enter Your Code here... You are welcome to add more cell if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13MhufByguGG"},"source":["### Task 1.2"]},{"cell_type":"code","metadata":{"id":"K5ps8JCRgyOl"},"source":["### TO DO - Enter Your Code here... You are welcome to add more cell if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzBTZUmdgv32"},"source":["### Task 1.3"]},{"cell_type":"markdown","metadata":{"id":"hWCofs2Xg7pu"},"source":["TO DO - Enter Your Response here\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z9sP7WC1372G"},"source":["## [Task 2] Implementing a More Complex RNN [30 pts]\n","\n","Replace the linear layer $g$ in the baseline model by a two-layer fully connected neural network with ReLU activation. The new subnetwork should implement:\n","\n","$$h^{(t)} = g(c^{(t)}) = \\sigma\\left(W_2 \\cdot \\sigma \\left(W_1 \\cdot c^{(t)} + b_1 \\right) + b_2 \\right),$$\n","\n","where $\\sigma$ is a ReLU activation, and $(W_k,b_k)$ are the parameters for a linear layer. Then, answer the following:\n","\n","1. [12 pts] Compare the learning curves for the baseline trained with Adam and this more complex model trained with Adam as well. Train both models for only 50,000 iterations.\n","2. [12 pts] Evaluate the performance of both models on the test set.\n","3. [6 pts] Comment on the performance of both methods on the test set, and the shape of their learning curves.\n"]},{"cell_type":"markdown","metadata":{"id":"sEoaFEuIiNhW"},"source":["### Task 2.1"]},{"cell_type":"code","metadata":{"id":"nFMrYpGXiUag"},"source":["### TO DO - Enter Your Code here... You are welcome to add more cell if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sf51O4_tiPXm"},"source":["### Task 2.2\n"]},{"cell_type":"code","metadata":{"id":"9XCQXvwIiUPg"},"source":["### TO DO - Enter Your Code here... You are welcome to add more cell if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vVlGo8oriRBe"},"source":["### Task 2.3"]},{"cell_type":"markdown","metadata":{"id":"8hFxqq_HiZPY"},"source":["TO DO - Enter Your Response here\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0LSiAtj3icSA"},"source":["## [Task 3] Using LSTM [40 pts]\n","\n","Replace the custom-built RNN for a standard LSTM layer. This may require you to do some significant changes to the network class and training functions.\n","\n","1. [16 pts] Compare the learning curves for the baseline trained with Adam and the LSTM model trained with Adam as well. Train both models for only 50,000 iterations.\n","2. [16 pts] Evaluate the performance of both models on the test set.\n","3. [8 pts] Comment on the performance of both methods on the test set, and the shape of their learning curves."]},{"cell_type":"markdown","metadata":{"id":"GNS2lvVxi9KT"},"source":["### Task 3.1"]},{"cell_type":"code","metadata":{"id":"ms7N8y3EjCpy"},"source":["### TO DO - Enter Your Code here... You are welcome to add more cell if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VS1549K3i_Jx"},"source":["### Task 3.2"]},{"cell_type":"code","metadata":{"id":"l4Qw8uzyjDfE"},"source":["### TO DO - Enter Your Code here... You are welcome to add more cell if needed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JeoLrw5HjAVc"},"source":["### Task 3.3"]},{"cell_type":"markdown","metadata":{"id":"q-kxb4eIjEQk"},"source":["TO DO - Enter Your Response here\n"]}]}