{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hw05.ipynb","provenance":[{"file_id":"1HAsn_X1X8MEiUBne_OuHDPHKUmaIZ9gr","timestamp":1634026313867},{"file_id":"1DDKNNUBHxVWifAekYT2onwSCAXAwmbjI","timestamp":1633979537915}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOdHB309fcKh2IscLMkW3CH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"U9wxy2h3pRpT"},"source":["# Exploring CNNs\n","\n","In this assignment, you will modify a baseline CNN architecture and explore different regularization an optimization schemes.\n","\n","We start by ensuring that we are using a GPU if available. If you are using Google Colab, make sure you change the Notebook Setting so a GPU is useed for Hardware Acceleration."]},{"cell_type":"code","metadata":{"id":"YwX9_CLEf6n-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634026114713,"user_tz":240,"elapsed":25092,"user":{"displayName":"Edgar Lobaton","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjB--zObKHnVqFEk21UoZuDEHTEYJ_DPb2kGtLNIg=s64","userId":"02856608958118334478"}},"outputId":"26fd0b7b-deb1-42f6-eabe-d87f2ca9ba65"},"source":["import torch\n","import numpy as np\n","\n","# Checking if CUDA is available\n","flag_cuda = torch.cuda.is_available()\n","\n","if not flag_cuda:\n","    print('Using CPU')\n","else:\n","    print('Using GPU')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU\n"]}]},{"cell_type":"markdown","metadata":{"id":"gG5TaXZrlq7f"},"source":["## Loading the Data\n","\n","We load the data, transform it to be the in the correct format, and create the training, validation and test sets.\n"]},{"cell_type":"code","metadata":{"id":"U5YYU7ecgG1y"},"source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# Size of the batch\n","batch_size = 20\n","# Percentage of training used for validation\n","valid_size = 0.2\n","\n","# Convert data to a normalized torch.FloatTensor\n","transform = transforms.Compose([\n","    # Converting RGB [0,255] to Tensor [0,1]\n","    transforms.ToTensor(),\n","    # Normalizes using specified mean and std per channel\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n","    ])\n","\n","# Selecting the training and test datasets\n","train_data = datasets.CIFAR10('data', train=True,\n","                              download=True, transform=transform)\n","test_data = datasets.CIFAR10('data', train=False,\n","                             download=True, transform=transform)\n","\n","# Getting training indices that will be used for validation\n","num_train = len(train_data)\n","idx = list(range(num_train))\n","np.random.shuffle(idx)\n","split = int(np.floor(valid_size * num_train))\n","train_idx, valid_idx = idx[split:], idx[:split]\n","\n","# Defining samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","# Preparing data loader\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","    sampler=train_sampler)\n","valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n","    sampler=valid_sampler)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n","\n","# specify the image classes\n","classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qKUlORMqpup"},"source":["## Defining the Model and Optimizer\n","\n","A class is created to specify the network structure."]},{"cell_type":"code","metadata":{"id":"j56kFEmRgsnl"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Defining the CNN architecture\n","class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.conv1 = nn.Conv2d(3, 6, 5)\n","    self.pool = nn.MaxPool2d(2, 2)\n","    self.conv2 = nn.Conv2d(6, 16, 5)\n","    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","    self.fc2 = nn.Linear(120, 84)\n","    self.fc3 = nn.Linear(84, 10)\n","  def forward(self, x):\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = self.pool(F.relu(self.conv2(x)))\n","    x = x.view(-1, 16 * 5 * 5) # At this point the feature map is 5 x 5 x 16\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    return x\n","\n","# Create a complete CNN\n","model = Net()\n","print(model)\n","\n","# Move tensors to GPU if CUDA is available\n","if flag_cuda:\n","  model.cuda()\n","\n","# Specifying the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Specify optimizer\n","optimizer = optim.SGD(model.parameters(), lr=.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g_MNr4LE3HJ7"},"source":["## Training the Model\n","\n","We define a helper function to make it easier test different models, criterions or optimizers. The best model trained is stored locally."]},{"cell_type":"code","metadata":{"id":"0MTMjBKGg5Zi"},"source":["import matplotlib.pyplot as plt\n","\n","# Specifying the number of epochs\n","n_epochs = 20\n","\n","def trainNet(model,criterion,optimizer,n_epochs,flag_cuda):\n","  # Unpacking the number of epochs to train the model\n","  epochs_list = [*range(1,n_epochs+1)]\n","\n","  # List to store loss to visualize\n","  train_losslist = []\n","  valid_losslist = []\n","  valid_loss_min = np.Inf # track change in validation loss\n","\n","  for epoch in epochs_list:\n","      # Keeping track of training and validation loss\n","      train_loss = 0.0\n","      valid_loss = 0.0\n","      \n","      ######################\n","      # Training the model #\n","      ######################\n","      model.train()\n","      for data, target in train_loader:\n","          # Moving tensors to GPU if CUDA is available\n","          if flag_cuda:\n","              data, target = data.cuda(), target.cuda()\n","          # Clearing the gradients of all optimized variables\n","          optimizer.zero_grad()\n","          # Forward pass: Computing predicted outputs\n","          output = model(data)\n","          # Calculating the batch loss\n","          loss = criterion(output, target)\n","          # Backward pass: compute gradient of loss with respect to parameters\n","          loss.backward()\n","          # Perform a single optimization step (parameter update)\n","          optimizer.step()\n","          # Update training loss\n","          train_loss += loss.item()*data.size(0)\n","          \n","      ########################    \n","      # Validating the model #\n","      ########################\n","      model.eval()\n","      for data, target in valid_loader:\n","          # Moving tensors to GPU if CUDA is available\n","          if flag_cuda:\n","              data, target = data.cuda(), target.cuda()\n","          output = model(data)\n","          loss = criterion(output, target)\n","          valid_loss += loss.item()*data.size(0)\n","      \n","      # Calculating average losses\n","      train_loss = train_loss/len(train_sampler)\n","      valid_loss = valid_loss/len(valid_sampler)\n","      train_losslist.append(train_loss)\n","      valid_losslist.append(valid_loss)\n","          \n","      # Printing training/validation statistics \n","      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss, valid_loss))\n","      \n","      # Saving model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","              valid_loss_min,valid_loss))\n","          torch.save(model.state_dict(), 'model_cifar.pt')\n","          valid_loss_min = valid_loss\n","        \n","  return epochs_list, train_losslist, valid_losslist\n","\n","# Executing the training\n","epochs_list, train_losslist, valid_losslist = trainNet(\n","    model,criterion,optimizer,n_epochs,flag_cuda)\n","\n","# Loading the best model\n","model.load_state_dict(torch.load('model_cifar.pt'))\n","\n","# Plotting the learning curves\n","plt.plot(epochs_list, train_losslist, epochs_list, valid_losslist)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['Training','Validation'])\n","plt.title(\"Performance of Baseline Model\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5_AQKiRLFHW"},"source":["## Evaluating Performance\n","\n","Here we use the test set for the final performance of the model. We define a helper function as well for ease of future evaluation of different models.\n"]},{"cell_type":"code","metadata":{"id":"XhobaYJviI5J"},"source":["def assessNet(model,criterion):\n","  # Tracking test loss and accuracy\n","  test_loss = 0.0\n","  class_correct = list(0. for i in range(len(classes)))\n","  class_total = list(0. for i in range(len(classes)))\n","\n","  # Setting model to evaluate\n","  model.eval()\n","\n","  # Iterating over batches of test data\n","  for data, target in test_loader:\n","      # Obtaining predictions and loss\n","      if flag_cuda:\n","          data, target = data.cuda(), target.cuda()\n","      output = model(data)\n","      loss = criterion(output, target)\n","      test_loss += loss.item()*data.size(0)\n","\n","      # Converting output probabilities to predicted class\n","      _, pred = torch.max(output, 1)    \n","      # Comparing predictions to true label\n","      correct_tensor = pred.eq(target.data.view_as(pred))\n","      correct = np.squeeze(correct_tensor.numpy()) if not flag_cuda else np.squeeze(correct_tensor.cpu().numpy())\n","      # Calculating test accuracy for each object class\n","      for i in range(batch_size):\n","          label = target.data[i]\n","          class_correct[label] += correct[i].item()\n","          class_total[label] += 1\n","\n","  # Computing the average test loss\n","  test_loss = test_loss/len(test_loader.dataset)\n","  print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","  # Computing the class accuracies\n","  for i in range(10):\n","      if class_total[i] > 0:\n","          print('Test Accuracy of %10s: %2d%% (%2d/%2d)' % (\n","              classes[i], 100 * class_correct[i] / class_total[i],\n","              np.sum(class_correct[i]), np.sum(class_total[i])))\n","      else:\n","          print('Test Accuracy of %10s: N/A (no training examples)' % (classes[i]))\n","\n","  # Computing the overall accuracy\n","  print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","      100. * np.sum(class_correct) / np.sum(class_total),\n","      np.sum(class_correct), np.sum(class_total)))\n","  \n","  return\n","\n","# Executing the assessment\n","assessNet(model,criterion)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5IyI6xrzDMv"},"source":["## **[Task 1] Comparing Optimizers** [50 pts]\n","\n","The goal of of this section is to compare the performance of various optimizers: SGD with momentum, RMS Prop and Adam. Make sure you complete the following steps:\n","\n","1. [10 pts] Train the network with SGD with momentum and perform the assessment\n","2. [10 pts] Train the network with RMS Prop and perform the assessment\n","3. [10 pts] Train the network with Adam and perform the assessment\n","4. [10 pts] Create a plot comparing training loss and another for validation loss for all the methods including your baseline\n","5. [10 pts] Comment on the performance of each optimizer. Which one had the steepest learning curve? Which one gave the final best performance in the validation and test sets? "]},{"cell_type":"code","metadata":{"id":"xua3QD6_zCn-"},"source":["# ADD - Training a new model with SGD with momentum\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvcrjMVJ1ngF"},"source":["# ADD - Training a new model with RMS Prop\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sa9W0hdH2C7S"},"source":["# ADD - Training a new model with Adam\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuRu6J832kjF"},"source":["# ADD - Plots Comparing training and validation loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6B0VwoqR33L-"},"source":["ADD - Comments "]},{"cell_type":"markdown","metadata":{"id":"z9sP7WC1372G"},"source":["## **[Task 2] Applying Standard Regularization** [50 pts]\n","\n","The goal of this section is to compare the effect of different normalization approaches including Batch normalization and Dropout. Make sure to complete the following steps:\n","\n","1. [15 pts] Create a new model by adding a batch normalization layer after each convolutional layer and between the dense layers of the base model. Train it using the Adam optimizer and perform the assessment.\n","2. [15 pts] Create a new model by adding dropout after each convolutional layer and between the dense layers of the base model with a dropout rate equal to $0.1$. Train it using the Adam optimizer and perform the assessment.\n","3. [10 pts] Create a plot comparing training loss and another for validation loss for the batch normalization and dropout versions against the base model trained with the Adam optimizer.\n","4. [10 pts] Comment on the performance of each model. Which one had the steepest learning curve? Which one gave the final best performance in the validation and test sets? "]},{"cell_type":"code","metadata":{"id":"tvYET6JO4Ngz"},"source":["# ADD - CNN architecture with batch normalization\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnUCCE7B9FIG"},"source":["# ADD - CNN architecture with dropout\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aF4IrjcL9IoQ"},"source":["# ADD - Plots Comparing training and validation loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3l8ajDSd_bfN"},"source":["ADD - Comments"]}]}